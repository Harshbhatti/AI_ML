{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64c30ea-b8af-4f09-9ee8-3eba46719f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6187821-e5e5-4f7b-8198-c13ab733718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d71ff6-1ccb-4c5f-becd-0eee10c49889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # You can use \"microsoft/DialoGPT-medium\" for a conversational model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate a chatbot response\n",
    "def generate_response(user_input):\n",
    "    inputs = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors=\"pt\")\n",
    "    reply_ids = model.generate(inputs, max_length=100, pad_token_id=tokenizer.eos_token_id)\n",
    "    response = tokenizer.decode(reply_ids[:, inputs.shape[-1]:][0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Simple loop to chat with the bot\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start chatting with the bot (type 'quit' to stop)!\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"quit\":\n",
    "            break\n",
    "        bot_response = generate_response(user_input)\n",
    "        print(f\"Bot: {bot_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3187d022-db2c-4584-b562-6a18e90b84f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cc5ae0-711f-4f6c-89eb-5e23f18ee801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d23ae92-6962-4fdd-97ef-a40668f65e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: torch in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (74.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Install Hugging Face Transformers and PyTorch\n",
    "!pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a77eef49-7644-43c8-9acb-0a95f2156a58",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2LMHeadModel, GPT2Tokenizer\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the pre-trained conversational model (DialoGPT)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\__init__.py:148\u001b[0m\n\u001b[0;32m    146\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    147\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 148\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    150\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained conversational model (DialoGPT)\n",
    "model_name = \"microsoft/DialoGPT-medium\"  # You can also try 'small' or 'large'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Function to generate a chatbot response\n",
    "def generate_response(user_input):\n",
    "    # Tokenize input and move it to the device (GPU or CPU)\n",
    "    inputs = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Create an attention mask (all 1s since no padding)\n",
    "    attention_mask = torch.ones(inputs.shape, device=device)\n",
    "    \n",
    "    # Generate response from the model\n",
    "    reply_ids = model.generate(\n",
    "        inputs, \n",
    "        attention_mask=attention_mask, \n",
    "        max_length=100, \n",
    "        pad_token_id=tokenizer.eos_token_id, \n",
    "        temperature=0.7,  # Controls randomness in generation\n",
    "        top_p=0.9,        # Nucleus sampling for more natural responses\n",
    "        do_sample=True    # Enable sampling for varied responses\n",
    "    )\n",
    "    \n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(reply_ids[:, inputs.shape[-1]:][0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Simple loop to chat with the bot\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start chatting with the bot (type 'quit' to stop)!\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"quit\":\n",
    "            break\n",
    "        bot_response = generate_response(user_input)\n",
    "        print(f\"Bot: {bot_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3305732-771f-469d-952c-480278d4b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "üöÄ Excited to share my latest AI chatbot project! I built this conversational bot using **Hugging Face Transformers** and **PyTorch**, powered by the \"microsoft/DialoGPT-medium\" model. üß†‚ú® This bot can have engaging and dynamic conversations, thanks to advanced natural language processing techniques. Whether it‚Äôs for customer support or fun interactions, it‚Äôs learning and improving with every chat! üí¨\n",
    "\n",
    "Check out the power of AI in real-time dialogue creation!\n",
    "\n",
    "#AI #Chatbot #NLP #DeepLearning #ConversationalAI #GPT2 #DialoGPT #Python #MachineLearning #HuggingFace #TechInnovation #AIChatbot #ArtificialIntelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ea2f1-84bd-47de-956d-33923003f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "üöÄ Introducing my new AI-powered chatbot built using Hugging Face Transformers and PyTorch! ü§ñ This conversational AI leverages the \"microsoft/DialoGPT-medium\" model to engage in natural and intelligent dialogues. Whether you're asking complex questions or just chatting, this bot delivers thoughtful responses using advanced NLP techniques like nucleus sampling and temperature tuning. üåêüí°\n",
    "\n",
    "Want to experience real-time AI conversations? Ask away! üôå\n",
    "\n",
    "#AI #Chatbot #NLP #GPT2 #HuggingFace #PyTorch #ArtificialIntelligence #ConversationalAI #MachineLearning #AIInnovation #TechForGood #DeepLearning #AIChatbot #AICommunity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
